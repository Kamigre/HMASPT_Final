{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff10898-dd63-464b-b74b-b1a07f2396d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Import libraries\n",
    "# -----------------------------\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import defeatbeta_api\n",
    "from defeatbeta_api.data.ticker import Ticker\n",
    "import numpy as np\n",
    "# import torch\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "start_date = pd.to_datetime(\"31-12-2017\", dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89f84b7e-7354-43e5-ab91-a2ad69f5e7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /usr/local/lib/python3.13/site-packages (6.3.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /usr/local/lib/python3.13/site-packages (from plotly) (2.10.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.13/site-packages (from plotly) (25.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.13/site-packages (from scikit-learn) (2.3.3)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9f138",
   "metadata": {},
   "source": [
    "# Constituent list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee02d54-23fe-4cc6-9e5c-0e9b9914f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Retrieve current S&P 500 constituents\n",
    "# -----------------------------\n",
    "def get_current_constituents():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies#Selected_changes_to_the_list_of_S&P_500_components\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # The 1st table contains the current list\n",
    "    df = pd.read_html(response.text, header=0)[0]\n",
    "    df = df[[\"Symbol\", \"Security\"]]\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Retrieve historical composition changes from Wikipedia\n",
    "# -----------------------------\n",
    "def get_historical_changes():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies#Selected_changes_to_the_list_of_S&P_500_components\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # The 2nd table contains the historical changes\n",
    "    changes = pd.read_html(response.text, header=[0, 1])[1]\n",
    "\n",
    "    # Flatten multi-level columns and normalize\n",
    "    changes.columns = [\" \".join(col).strip() for col in changes.columns.values]\n",
    "    changes = changes.rename(columns={\"Effective Date Effective Date\": \"Date\"})\n",
    "    changes[\"Date\"] = pd.to_datetime(changes[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "    return changes.dropna(subset=[\"Date\"]).sort_values(\"Date\")\n",
    "\n",
    "# -----------------------------\n",
    "# Build a timeline of index members over time\n",
    "# -----------------------------\n",
    "def build_constituents_timeline(current, changes):\n",
    "    members = set(current[\"Symbol\"])\n",
    "    timeline = defaultdict(set)\n",
    "\n",
    "    # Iterate backwards to reconstruct past compositions\n",
    "    for _, row in changes.sort_values(\"Date\", ascending=False).iterrows():\n",
    "        date = row[\"Date\"]\n",
    "        if pd.notna(row.get(\"Added Ticker\")):\n",
    "            members.discard(row[\"Added Ticker\"])\n",
    "        if pd.notna(row.get(\"Removed Ticker\")):\n",
    "            members.add(row[\"Removed Ticker\"])\n",
    "        timeline[date.strftime(\"%d-%m-%Y\")] = members.copy()\n",
    "\n",
    "    return dict(sorted(timeline.items()))\n",
    "\n",
    "# -----------------------------\n",
    "# Save timeline as JSON\n",
    "# -----------------------------\n",
    "def save_timeline(timeline, filename=\"sp500_timeline.json\"):\n",
    "    serializable = {d: sorted(list(tickers)) for d, tickers in timeline.items()}\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(serializable, f, indent=2)\n",
    "    print(f\"Timeline saved to {filename}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Main execution\n",
    "# -----------------------------\n",
    "current = get_current_constituents()\n",
    "changes = get_historical_changes()\n",
    "timeline = build_constituents_timeline(current, changes)\n",
    "save_timeline(timeline)\n",
    "\n",
    "# -----------------------------\n",
    "# Load and analyze timeline\n",
    "# -----------------------------\n",
    "with open(\"sp500_timeline.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "timeline = {datetime.strptime(k, \"%d-%m-%Y\"): set(v) for k, v in data.items()}\n",
    "if not timeline:\n",
    "    raise SystemExit(\"No snapshots found in sp500_timeline.json\")\n",
    "\n",
    "# Filter snapshots from the last 5 years\n",
    "cutoff_date = datetime.now() - timedelta(days=5*365)\n",
    "recent_snapshots = [s for d, s in timeline.items() if d >= cutoff_date and s]\n",
    "\n",
    "if not recent_snapshots:\n",
    "    raise SystemExit(\"No snapshots found in the last 5 years\")\n",
    "\n",
    "# Compute tickers present in all recent snapshots\n",
    "present_last_5_years = set.intersection(*recent_snapshots)\n",
    "\n",
    "print(f\"Tickers consistently present in the last 5 years: {len(present_last_5_years)}\")\n",
    "print(sorted(present_last_5_years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ee79ae-efa3-4538-8a1c-eb8ab234f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Ticker list\n",
    "# -----------------------------\n",
    "\n",
    "# List of tickers\n",
    "tickers = ['A', 'AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADSK', 'AEE', 'AEP', 'AES', 'AFL', 'AIG', 'AIZ', 'AJG', 'AKAM',\n",
    "           'ALB', 'ALGN', 'ALL', 'ALLE', 'AMAT', 'AMCR', 'AMD', 'AME', 'AMGN', 'AMP', 'AMT', 'AMZN', 'ANET', 'AON', 'AOS', 'APA', 'APD', 'APH',\n",
    "           'APTV', 'ARE', 'ATO', 'AVB', 'AVGO', 'AVY', 'AWK', 'AXP', 'AZO', 'BA', 'BAC', 'BALL', 'BAX', 'BBY', 'BDX', 'BEN', 'BF.B', 'BIIB', 'BK',\n",
    "           'BKNG', 'BKR', 'BLK', 'BMY', 'BR', 'BRK.B', 'BSX', 'BXP', 'C', 'CAG', 'CAH', 'CARR', 'CAT', 'CB', 'CBOE', 'CBRE', 'CCI', 'CCL', 'CDNS',\n",
    "           'CDW', 'CF', 'CFG', 'CHD', 'CHRW', 'CHTR', 'CI', 'CINF', 'CL', 'CLX', 'CMCSA', 'CME', 'CMG', 'CMI', 'CMS', 'CNC', 'CNP', 'COF', 'COO', 'COP',\n",
    "           'COR', 'COST', 'CPAY', 'CPB', 'CPRT', 'CRM', 'CSCO', 'CSX', 'CTAS', 'CTRA', 'CTSH', 'CTVA', 'CVS', 'CVX', 'D', 'DAL', 'DAY', 'DD', 'DE', 'DG',\n",
    "           'DGX', 'DHI', 'DHR', 'DIS', 'DLR', 'DLTR', 'DOC', 'DOV', 'DOW', 'DPZ', 'DRI', 'DTE', 'DUK', 'DVA', 'DVN', 'DXCM', 'EA', 'EBAY', 'ECL', 'ED', 'EFX',\n",
    "           'EG', 'EIX', 'EL', 'ELV', 'EMN', 'EMR', 'EOG', 'EQIX', 'EQR', 'ES', 'ESS', 'ETN', 'ETR', 'EVRG', 'EW', 'EXC', 'EXPD', 'EXPE', 'EXR', 'F', 'FANG',\n",
    "           'FAST', 'FCX', 'FDX', 'FE', 'FFIV', 'FI', 'FIS', 'FITB', 'FOX', 'FOXA', 'FRT', 'FTNT', 'FTV', 'GD', 'GE', 'GEN', 'GILD', 'GIS', 'GL', 'GLW', 'GM',\n",
    "           'GOOG', 'GOOGL', 'GPC', 'GPN', 'GRMN', 'GS', 'GWW', 'HAL', 'HAS', 'HBAN', 'HCA', 'HD', 'HIG', 'HII', 'HLT', 'HOLX', 'HON', 'HPE', 'HPQ', 'HRL', 'HSIC',\n",
    "           'HST', 'HSY', 'HUM', 'HWM', 'IBM', 'ICE', 'IDXX', 'IEX', 'IFF', 'INCY', 'INTC', 'INTU', 'IP', 'IPG', 'IQV', 'IR', 'IRM', 'ISRG', 'IT', 'ITW', 'IVZ', 'J',\n",
    "           'JBHT', 'JCI', 'JKHY', 'JNJ', 'JPM', 'K', 'KEY', 'KEYS', 'KHC', 'KIM', 'KLAC', 'KMB', 'KMI', 'KMX', 'KO', 'KR', 'L', 'LDOS', 'LEN', 'LH', 'LHX', 'LIN',\n",
    "           'LKQ', 'LLY', 'LMT', 'LNT', 'LOW', 'LRCX', 'LUV', 'LVS', 'LW', 'LYB', 'LYV', 'MA', 'MAA', 'MAR', 'MAS', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET',\n",
    "           'META', 'MGM', 'MHK', 'MKC', 'MKTX', 'MLM', 'MMC', 'MMM', 'MNST', 'MO', 'MOS', 'MPC', 'MRK', 'MS', 'MSCI', 'MSFT', 'MSI', 'MTB', 'MTD', 'MU', 'NCLH', 'NDAQ',\n",
    "           'NEE', 'NEM', 'NFLX', 'NI', 'NKE', 'NOC', 'NOW', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NVR', 'NWS', 'NWSA', 'O', 'ODFL', 'OKE', 'OMC', 'ORCL',\n",
    "           'ORLY', 'OTIS', 'OXY', 'PAYC', 'PAYX', 'PCAR', 'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PLD', 'PM', 'PNC', 'PNR', 'PNW', 'POOL',\n",
    "           'PPG', 'PPL', 'PRU', 'PSA', 'PSKY', 'PSX', 'PWR', 'PYPL', 'QCOM', 'RCL', 'REG', 'REGN', 'RF', 'RJF', 'RL', 'RMD', 'ROK', 'ROL', 'ROP', 'ROST', 'RSG', 'RTX',\n",
    "           'RVTY', 'SBAC', 'SBUX', 'SCHW', 'SHW', 'SJM', 'SLB', 'SNA', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRE', 'STE', 'STT', 'STX', 'STZ', 'SW', 'SWK', 'SWKS', 'SYF',\n",
    "           'SYK', 'SYY', 'T', 'TAP', 'TDG', 'TDY', 'TEL', 'TER', 'TFC', 'TGT', 'TJX', 'TMO', 'TMUS', 'TPR', 'TROW', 'TRV', 'TSCO', 'TSN', 'TT', 'TTWO', 'TXN', 'TXT',\n",
    "           'TYL', 'UAL', 'UDR', 'UHS', 'ULTA', 'UNH', 'UNP', 'UPS', 'URI', 'USB', 'V', 'VLO', 'VMC', 'VRSK', 'VRSN', 'VRTX', 'VTR', 'VTRS', 'VZ', 'WAB', 'WAT', 'WDC',\n",
    "           'WEC', 'WELL', 'WFC', 'WM', 'WMB', 'WMT', 'WRB', 'WST', 'WTW', 'WY', 'WYNN', 'XEL', 'XOM', 'XYL', 'YUM', 'ZBH', 'ZBRA', 'ZTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8817a45-1459-4f64-89bd-043acf9853bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 433/433 [09:22<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All merged data since 2017-12-31 stored in all_data.json\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Price adjustments helper\n",
    "# -----------------------------\n",
    "def parse_split_factor(factor):\n",
    "    if factor is None:\n",
    "        return 1.0\n",
    "    if isinstance(factor, str) and \":\" in factor:\n",
    "        num, denom = factor.split(\":\")\n",
    "        return float(num) / float(denom)\n",
    "    return float(factor)\n",
    "\n",
    "# -----------------------------\n",
    "# Collect and store all data\n",
    "# -----------------------------\n",
    "\n",
    "def collect_all_data(tickers, start_date, output_path=\"all_data.json\"):\n",
    "\n",
    "    all_data = {}\n",
    "\n",
    "    for tick in tqdm(tickers):\n",
    "        ticker_obj = Ticker(tick)\n",
    "\n",
    "        # ---------- PRICE DATA ----------\n",
    "        prices_df = ticker_obj.price()\n",
    "        if prices_df is None or prices_df.empty:\n",
    "            continue\n",
    "        prices_df[\"report_date\"] = pd.to_datetime(prices_df[\"report_date\"])\n",
    "        prices_df = prices_df[prices_df[\"report_date\"] >= start_date]\n",
    "        prices_df.set_index(\"report_date\", inplace=True)\n",
    "        closes = prices_df[\"close\"].sort_index()\n",
    "        volumes = prices_df[\"volume\"].sort_index()\n",
    "        if closes.empty:\n",
    "            continue\n",
    "\n",
    "        # Splits\n",
    "        splits_df = ticker_obj.splits()\n",
    "        splits_dict = {}\n",
    "        if splits_df is not None and not splits_df.empty:\n",
    "            splits_df[\"report_date\"] = pd.to_datetime(splits_df[\"report_date\"])\n",
    "            splits_df = splits_df[splits_df[\"report_date\"] >= start_date]\n",
    "            splits_df.set_index(\"report_date\", inplace=True)\n",
    "            splits_df[\"split_factor\"] = splits_df[\"split_factor\"].apply(parse_split_factor)\n",
    "            splits_dict = splits_df[\"split_factor\"].to_dict()\n",
    "\n",
    "        # Dividends\n",
    "        dividends_df = ticker_obj.dividends()\n",
    "        dividends_dict = {}\n",
    "        if dividends_df is not None and not dividends_df.empty:\n",
    "            dividends_df[\"report_date\"] = pd.to_datetime(dividends_df[\"report_date\"])\n",
    "            dividends_df = dividends_df[dividends_df[\"report_date\"] >= start_date]\n",
    "            dividends_df.set_index(\"report_date\", inplace=True)\n",
    "            dividends_dict = dividends_df[\"amount\"].to_dict()\n",
    "\n",
    "        # Dividend-adjustment factors\n",
    "        adj_factor = 1.0\n",
    "        adj_factors = {}\n",
    "        for date in reversed(sorted(closes.index)):\n",
    "            price = float(closes[date])\n",
    "            div_amt = float(dividends_dict.get(date, 0.0))\n",
    "            adj_factors[date] = adj_factor\n",
    "            if div_amt != 0.0 and price > 0:\n",
    "                adj_factor *= price / (price - div_amt)\n",
    "\n",
    "        adj_closes = {date: float(closes[date]) / adj_factors[date] for date in closes.index}\n",
    "\n",
    "        # Base prices DataFrame\n",
    "        prices_data = pd.DataFrame({\n",
    "            \"symbol\": tick,\n",
    "            \"report_date\": closes.index,\n",
    "            \"close\": closes.values,\n",
    "            \"adj_close\": [adj_closes[d] for d in closes.index],\n",
    "            \"adj_factor\": [adj_factors[d] for d in closes.index],\n",
    "            \"split_factor\": [splits_dict.get(d, 1.0) for d in closes.index],\n",
    "            \"div_amount\": [dividends_dict.get(d, 0.0) for d in closes.index],\n",
    "            \"volume\": volumes.values\n",
    "        })\n",
    "\n",
    "        # ---------- QUARTERLY FUNDAMENTALS ----------\n",
    "        eps_df = ticker_obj.quarterly_ttm_eps_yoy_growth()\n",
    "        if eps_df is not None and not eps_df.empty:\n",
    "            eps_df[\"report_date\"] = pd.to_datetime(eps_df[\"report_date\"])\n",
    "            eps_df = eps_df[eps_df[\"report_date\"] >= start_date]\n",
    "            eps_df[\"symbol\"] = tick\n",
    "\n",
    "            prices_data = prices_data.merge(\n",
    "                eps_df,\n",
    "                on=[\"symbol\", \"report_date\"],\n",
    "                how=\"left\"\n",
    "            )\n",
    "\n",
    "            # Forward-fill EPS values\n",
    "            prices_data[\"eps_yoy_growth\"] = prices_data.groupby(\"symbol\")[\"yoy_growth\"].ffill()\n",
    "\n",
    "        prices_data = prices_data.sort_values([\"symbol\", \"report_date\"])\n",
    "\n",
    "        # ---------- FIXED: forward/backward fill ----------\n",
    "        non_group_cols = [c for c in prices_data.columns if c not in [\"symbol\", \"report_date\"]]\n",
    "        prices_data[non_group_cols] = prices_data.groupby(\"symbol\", group_keys=False)[non_group_cols].apply(lambda x: x.ffill().bfill())\n",
    "\n",
    "        # ---------- PEG ratio using adj_close ----------\n",
    "        peg_df = ticker_obj.peg_ratio()\n",
    "        if peg_df is not None and not peg_df.empty:\n",
    "            peg_df[\"report_date\"] = pd.to_datetime(peg_df[\"report_date\"])\n",
    "            prices_data[\"report_date\"] = prices_data[\"report_date\"].dt.normalize()\n",
    "            peg_df[\"report_date\"] = peg_df[\"report_date\"].dt.normalize()\n",
    "            peg_df[\"symbol\"] = tick\n",
    "\n",
    "            peg_merged = peg_df.merge(\n",
    "                prices_data[[\"symbol\", \"report_date\", \"adj_close\"]],\n",
    "                on=[\"symbol\", \"report_date\"],\n",
    "                how=\"left\"\n",
    "            )\n",
    "\n",
    "            peg_merged[\"peg_adj\"] = peg_merged[\"adj_close\"] / peg_merged[\"ttm_eps\"] / (peg_merged[\"eps_yoy_growth\"] * 100)\n",
    "            peg_merged.loc[\n",
    "                (peg_merged[\"ttm_eps\"].isna()) | (peg_merged[\"eps_yoy_growth\"].isna()) | (peg_merged[\"eps_yoy_growth\"] == 0),\n",
    "                \"peg_adj\"\n",
    "            ] = None\n",
    "\n",
    "            prices_data = prices_data.merge(\n",
    "                peg_merged[[\"symbol\", \"report_date\", \"peg_adj\"]],\n",
    "                on=[\"symbol\", \"report_date\"],\n",
    "                how=\"left\"\n",
    "            )\n",
    "            prices_data[\"peg_adj\"] = prices_data.groupby(\"symbol\")[\"peg_adj\"].ffill()\n",
    "\n",
    "        prices_data = prices_data.loc[:, ~prices_data.columns.duplicated()]\n",
    "\n",
    "        # ---------- METADATA ----------\n",
    "        sector = ticker_obj.info().get(\"sector\")[0]\n",
    "        prices_data[\"sector\"] = sector\n",
    "\n",
    "        # ---------- STORE (flat JSON structure) ----------\n",
    "        ticker_data = {\n",
    "            \"sector\": sector,\n",
    "            \"data\": []\n",
    "        }\n",
    "\n",
    "        for _, row in prices_data.iterrows():\n",
    "            entry = {\n",
    "                \"date\": row[\"report_date\"].strftime(\"%Y-%m-%d\"),\n",
    "                \"close\": float(row[\"close\"]),\n",
    "                \"adj_close\": float(row[\"adj_close\"]),\n",
    "                \"adj_factor\": float(row[\"adj_factor\"]),\n",
    "                \"split_factor\": float(row[\"split_factor\"]),\n",
    "                \"div_amount\": float(row[\"div_amount\"]),\n",
    "                \"volume\": float(row[\"volume\"]),\n",
    "                \"eps_yoy_growth\": (\n",
    "                    None if pd.isna(row.get(\"eps_yoy_growth\")) else float(row[\"eps_yoy_growth\"])\n",
    "                ),\n",
    "                \"peg_adj\": (\n",
    "                    None if pd.isna(row.get(\"peg_adj\")) else float(row[\"peg_adj\"])\n",
    "                )\n",
    "            }\n",
    "            ticker_data[\"data\"].append(entry)\n",
    "\n",
    "        all_data[tick] = ticker_data\n",
    "\n",
    "    # ---------- SAVE JSON ----------\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"All merged data since {start_date.strftime('%Y-%m-%d')} stored in {output_path}\")\n",
    "    \n",
    "collect_all_data(tickers, start_date, output_path=\"all_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3911ce0-c17d-4a47-9e71-24990acdbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT DATA FROM JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5cc84fa-d82e-46bf-9a0c-27ae41662699",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "records = []\n",
    "for ticker, content in data.items():\n",
    "    sector = content.get(\"sector\", None)\n",
    "    for rec in content.get(\"data\", []):\n",
    "        rec[\"ticker\"] = ticker\n",
    "        rec[\"sector\"] = sector\n",
    "        # Reorder keys so 'date' appears first\n",
    "        rec = {\"date\": rec[\"date\"], **{k: v for k, v in rec.items() if k != \"date\"}}\n",
    "        records.append(rec)\n",
    "\n",
    "df = pd.json_normalize(records)\n",
    "\n",
    "# Convert 'date' to datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# Ensure column order explicitly (in case JSON order changes)\n",
    "cols = [\"date\"] + [c for c in df.columns if c != \"date\"]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e045211-4e9c-4035-8c17-0249b2a43e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983fb71-8ca5-44f8-a5ec-4ad05186277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dates from 2020-01-01 onwards\n",
    "df = df[df[\"date\"] >= \"2020-01-01\"]\n",
    "\n",
    "# Count how many unique dates each ticker has\n",
    "ticker_date_counts = df.groupby(\"ticker\")[\"report_date\"].nunique().reset_index()\n",
    "ticker_date_counts.rename(columns={\"date\": \"num_dates\"}, inplace=True)\n",
    "\n",
    "# Group tickers by number of dates\n",
    "grouped = ticker_date_counts.groupby(\"num_dates\")[\"ticker\"].apply(list).reset_index()\n",
    "\n",
    "# Sort by number of dates\n",
    "grouped = grouped.sort_values(by=\"num_dates\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display\n",
    "for _, row in grouped.iterrows():\n",
    "    print(f\"{row['num_dates']} dates: {row['ticker']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67277b36-8955-4b2d-839e-1a375fe221cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427\n",
      "['A', 'AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADSK', 'AEE', 'AEP', 'AES', 'AFL', 'AIG', 'AIZ', 'AJG', 'AKAM', 'ALB', 'ALGN', 'ALL', 'ALLE', 'AMAT', 'AMCR', 'AMD', 'AME', 'AMGN', 'AMP', 'AMT', 'AMZN', 'ANET', 'AON', 'AOS', 'APA', 'APD', 'APH', 'APTV', 'ARE', 'ATO', 'AVB', 'AVGO', 'AVY', 'AWK', 'AXP', 'AZO', 'BA', 'BAC', 'BALL', 'BAX', 'BBY', 'BDX', 'BEN', 'BIIB', 'BK', 'BKNG', 'BKR', 'BLK', 'BMY', 'BR', 'BSX', 'BXP', 'C', 'CAG', 'CAH', 'CAT', 'CB', 'CBOE', 'CBRE', 'CCI', 'CCL', 'CDNS', 'CDW', 'CF', 'CFG', 'CHD', 'CHRW', 'CHTR', 'CI', 'CINF', 'CL', 'CLX', 'CMCSA', 'CME', 'CMG', 'CMI', 'CMS', 'CNC', 'CNP', 'COF', 'COO', 'COP', 'COR', 'COST', 'CPAY', 'CPB', 'CPRT', 'CRM', 'CSCO', 'CSX', 'CTAS', 'CTRA', 'CTSH', 'CTVA', 'CVS', 'CVX', 'D', 'DAL', 'DAY', 'DD', 'DE', 'DG', 'DGX', 'DHI', 'DHR', 'DIS', 'DLR', 'DLTR', 'DOC', 'DOV', 'DOW', 'DPZ', 'DRI', 'DTE', 'DUK', 'DVA', 'DVN', 'DXCM', 'EA', 'EBAY', 'ECL', 'ED', 'EFX', 'EG', 'EIX', 'EL', 'ELV', 'EMN', 'EMR', 'EOG', 'EQIX', 'EQR', 'ES', 'ESS', 'ETN', 'ETR', 'EVRG', 'EW', 'EXC', 'EXPD', 'EXPE', 'EXR', 'F', 'FANG', 'FAST', 'FCX', 'FDX', 'FE', 'FFIV', 'FI', 'FIS', 'FITB', 'FOX', 'FOXA', 'FRT', 'FTNT', 'FTV', 'GD', 'GE', 'GEN', 'GILD', 'GIS', 'GL', 'GLW', 'GM', 'GOOG', 'GOOGL', 'GPC', 'GPN', 'GRMN', 'GS', 'GWW', 'HAL', 'HAS', 'HBAN', 'HCA', 'HD', 'HIG', 'HII', 'HLT', 'HOLX', 'HON', 'HPE', 'HPQ', 'HRL', 'HSIC', 'HST', 'HSY', 'HUM', 'HWM', 'IBM', 'ICE', 'IDXX', 'IEX', 'IFF', 'INCY', 'INTC', 'INTU', 'IP', 'IPG', 'IQV', 'IR', 'IRM', 'ISRG', 'IT', 'ITW', 'IVZ', 'J', 'JBHT', 'JCI', 'JKHY', 'JNJ', 'JPM', 'K', 'KEY', 'KEYS', 'KHC', 'KIM', 'KLAC', 'KMB', 'KMI', 'KMX', 'KO', 'KR', 'L', 'LDOS', 'LEN', 'LH', 'LHX', 'LIN', 'LKQ', 'LLY', 'LMT', 'LNT', 'LOW', 'LRCX', 'LUV', 'LVS', 'LW', 'LYB', 'LYV', 'MA', 'MAA', 'MAR', 'MAS', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'META', 'MGM', 'MHK', 'MKC', 'MKTX', 'MLM', 'MMC', 'MMM', 'MNST', 'MO', 'MOS', 'MPC', 'MRK', 'MS', 'MSCI', 'MSFT', 'MSI', 'MTB', 'MTD', 'MU', 'NCLH', 'NDAQ', 'NEE', 'NEM', 'NFLX', 'NI', 'NKE', 'NOC', 'NOW', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NVR', 'NWS', 'NWSA', 'O', 'ODFL', 'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYC', 'PAYX', 'PCAR', 'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PLD', 'PM', 'PNC', 'PNR', 'PNW', 'POOL', 'PPG', 'PPL', 'PRU', 'PSA', 'PSX', 'PWR', 'PYPL', 'QCOM', 'RCL', 'REG', 'REGN', 'RF', 'RJF', 'RL', 'RMD', 'ROK', 'ROL', 'ROP', 'ROST', 'RSG', 'RTX', 'RVTY', 'SBAC', 'SBUX', 'SCHW', 'SHW', 'SJM', 'SLB', 'SNA', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRE', 'STE', 'STT', 'STX', 'STZ', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYY', 'T', 'TAP', 'TDG', 'TDY', 'TEL', 'TER', 'TFC', 'TGT', 'TJX', 'TMO', 'TMUS', 'TPR', 'TROW', 'TRV', 'TSCO', 'TSN', 'TT', 'TTWO', 'TXN', 'TXT', 'TYL', 'UAL', 'UDR', 'UHS', 'ULTA', 'UNH', 'UNP', 'UPS', 'URI', 'USB', 'V', 'VLO', 'VMC', 'VRSK', 'VRSN', 'VRTX', 'VTR', 'VTRS', 'VZ', 'WAB', 'WAT', 'WDC', 'WEC', 'WELL', 'WFC', 'WM', 'WMB', 'WMT', 'WRB', 'WST', 'WTW', 'WY', 'WYNN', 'XEL', 'XOM', 'XYL', 'YUM', 'ZBH', 'ZBRA', 'ZTS']\n"
     ]
    }
   ],
   "source": [
    "# Ensure report_date is datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# Define cutoff date\n",
    "cutoff = pd.Timestamp(\"2020-01-01\")\n",
    "\n",
    "# Filter records after cutoff\n",
    "df_filtered = df[df[\"date\"] >= cutoff]\n",
    "\n",
    "# Count total records per ticker\n",
    "summary_df = df_filtered.groupby(\"ticker\").size().reset_index(name=\"total_records\")\n",
    "\n",
    "# Find tickers with maximum records\n",
    "max_records = summary_df[\"total_records\"].max()\n",
    "tickers_maxdays = summary_df[summary_df[\"total_records\"] == max_records][\"ticker\"].tolist()\n",
    "\n",
    "print(len(tickers_maxdays))\n",
    "print(tickers_maxdays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0781cfc8-e366-4311-8fc4-43bb13a77d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ticker  avg_volume_million\n",
      "0    NVDA          417.091831\n",
      "1    AAPL           85.886052\n",
      "2       F           71.486191\n",
      "3    AMZN           65.180831\n",
      "4     AMD           61.852816\n",
      "..    ...                 ...\n",
      "95   NFLX            6.005586\n",
      "96    DOW            5.970817\n",
      "97    APH            5.930738\n",
      "98    ABT            5.802972\n",
      "99   MCHP            5.802927\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ensure report_date is a datetime\n",
    "df[\"report_date\"] = pd.to_datetime(df[\"report_date\"])\n",
    "\n",
    "# Define cutoff date\n",
    "cutoff = pd.Timestamp(\"2020-01-01\")\n",
    "\n",
    "# Filter the DataFrame for dates after cutoff\n",
    "df_filtered = df[df[\"report_date\"] >= cutoff]\n",
    "\n",
    "# Compute average daily volume per ticker\n",
    "avg_volume_df = df_filtered.groupby(\"ticker\")[\"volume\"].mean().reset_index()\n",
    "avg_volume_df.rename(columns={\"volume\": \"avg_volume\"}, inplace=True)\n",
    "\n",
    "# Drop missing values if any\n",
    "avg_volume_df = avg_volume_df.dropna()\n",
    "\n",
    "# Sort by average volume and keep top 100\n",
    "top100_df = avg_volume_df.sort_values(\"avg_volume\", ascending=False).head(100)\n",
    "\n",
    "# Optional: convert to millions for readability\n",
    "top100_df[\"avg_volume_million\"] = top100_df[\"avg_volume\"] / 1_000_000\n",
    "top100_df = top100_df.reset_index(drop=True)\n",
    "\n",
    "print(top100_df[[\"ticker\", \"avg_volume_million\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56bb8dbd-dc2e-450b-b528-083134158920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 100 by volume\n",
    "\n",
    "tickers_maxdays = [\n",
    "    'NVDA', 'AAPL', 'F', 'AMZN', 'AMD', 'INTC', 'BAC', 'T', 'CCL', 'PFE',\n",
    "    'GOOGL', 'MSFT', 'GOOG', 'AVGO', 'WFC', 'META', 'WMT', 'XOM', 'CMCSA', 'CSCO',\n",
    "    'MU', 'VZ', 'C', 'NCLH', 'OXY', 'FCX', 'HBAN', 'CMG', 'KO', 'GM',\n",
    "    'KMI', 'DAL', 'LRCX', 'UAL', 'CSX', 'HPE', 'KEY', 'SLB', 'JPM', 'PYPL',\n",
    "    'BMY', 'BA', 'DIS', 'HAL', 'MRK', 'AMCR', 'ORCL', 'GE', 'DVN', 'SCHW',\n",
    "    'CVX', 'ANET', 'NEE', 'VTRS', 'HPQ', 'RF', 'NKE', 'LUV', 'USB', 'MO',\n",
    "    'NEM', 'QCOM', 'MS', 'SBUX', 'GILD', 'HST', 'CVS', 'APA', 'JNJ', 'KHC',\n",
    "    'WMB', 'BSX', 'BKR', 'TFC', 'COP', 'CTRA', 'MGM', 'V', 'AES', 'EXC',\n",
    "    'AMAT', 'PG', 'ORLY', 'MDLZ', 'KR', 'CRM', 'EBAY', 'FAST', 'WDC', 'ABBV',\n",
    "    'RTX', 'LVS', 'TJX', 'FTNT', 'MDT', 'NFLX', 'DOW', 'APH', 'ABT', 'MCHP'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "555466f0-60d9-405f-8ea3-091e67d05271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Containers for ticker data\n",
    "# -------------------------------\n",
    "sectors = []\n",
    "industries = []\n",
    "market_caps = []\n",
    "\n",
    "# -------------------------------\n",
    "# Fetch info and summary for all tickers\n",
    "# -------------------------------\n",
    "for symbol in tickers_maxdays:\n",
    "    try:\n",
    "        ticker = Ticker(symbol)\n",
    "        info = ticker.info()[[\"sector\", \"industry\"]].to_dict(orient=\"records\")[0]\n",
    "        summary = ticker.summary().to_dict(orient=\"records\")[0]\n",
    "\n",
    "        sectors.append(info.get(\"sector\"))\n",
    "        market_caps.append(summary.get(\"market_cap\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {symbol}: {e}\")\n",
    "        sectors.append(None)\n",
    "        industries.append(None)\n",
    "        market_caps.append(None)\n",
    "\n",
    "ticker_info_df = pd.DataFrame({\n",
    "    \"ticker\": tickers_maxdays,\n",
    "    \"sector\": sectors,\n",
    "})\n",
    "\n",
    "market_cap_df = pd.DataFrame({\n",
    "    \"ticker\": tickers_maxdays,\n",
    "    \"market_cap\": market_caps\n",
    "}).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1000102c-38b3-4dc3-8eab-f4b6fe86d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Count of stocks by sector\n",
    "# -------------------------------\n",
    "# Count of stocks by sector\n",
    "sector_count = ticker_info_df[\"sector\"].value_counts().reset_index()\n",
    "sector_count.columns = [\"sector\", \"count\"]\n",
    "\n",
    "fig_sector = px.bar(\n",
    "    sector_count,\n",
    "    x=\"sector\",\n",
    "    y=\"count\",\n",
    "    text=\"count\",\n",
    "    color_discrete_sequence=[\"#4C72B0\"]\n",
    ")\n",
    "\n",
    "fig_sector.update_layout(\n",
    "    showlegend=False,\n",
    "    font=dict(family=\"Calibri\", size=20),\n",
    "    plot_bgcolor=\"white\",\n",
    "    paper_bgcolor=\"white\",\n",
    "    margin=dict(t=10),\n",
    "    xaxis=dict(showgrid=False, showline=True, linecolor=\"black\"),\n",
    "    yaxis=dict(showgrid=True, gridcolor=\"#D3D3D3\", zeroline=False, showline=True, linecolor=\"black\"),\n",
    "    xaxis_title=\"Sector\",\n",
    "    yaxis_title=\"Number of stocks\"\n",
    ")\n",
    "\n",
    "fig_sector.update_traces(textposition=\"outside\")\n",
    "fig_sector.write_html(\"sector_distribution.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "605ff143-fcb2-43d1-a261-8579137da621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Market capitalization distribution (log-scaled)\n",
    "# -------------------------------\n",
    "\n",
    "# Filter positive market caps and log-transform\n",
    "market_cap_df = market_cap_df[market_cap_df[\"market_cap\"] > 0].copy()\n",
    "market_cap_df[\"log_market_cap\"] = np.log10(market_cap_df[\"market_cap\"])\n",
    "\n",
    "fig_market_cap = px.histogram(\n",
    "    market_cap_df,\n",
    "    x=\"log_market_cap\",\n",
    "    nbins=80,\n",
    "    color_discrete_sequence=[\"#C44E52\"]\n",
    ")\n",
    "\n",
    "fig_market_cap.update_layout(\n",
    "    showlegend=False,\n",
    "    font=dict(family=\"Calibri\", size=20),\n",
    "    plot_bgcolor=\"white\",\n",
    "    paper_bgcolor=\"white\",\n",
    "    margin=dict(t=10),\n",
    "    xaxis=dict(showgrid=False, showline=True, linecolor=\"black\"),\n",
    "    yaxis=dict(showgrid=True, gridcolor=\"#D3D3D3\", zeroline=False, showline=True, linecolor=\"black\"),\n",
    "    xaxis_title=\"Log of market capitalization (USD)\",\n",
    "    yaxis_title=\"Number of stocks\"\n",
    ")\n",
    "\n",
    "fig_market_cap.write_html(\"market_cap_distribution.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14379970-86c5-4c12-a601-b2ce285e8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Daily volume\n",
    "# -------------------------------\n",
    "\n",
    "# Ensure report_date is datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# Filter by date and selected tickers\n",
    "cutoff = pd.Timestamp(\"2020-01-01\")\n",
    "df_filtered = df[(df[\"date\"] >= cutoff) & (df[\"ticker\"].isin(tickers_maxdays))]\n",
    "\n",
    "# Compute average daily volume in millions\n",
    "avg_volume_df = (\n",
    "    df_filtered.groupby(\"ticker\")[\"volume\"]\n",
    "    .mean()\n",
    "    .div(1_000_000)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"volume\": \"avg_volume_million\"})\n",
    ")\n",
    "\n",
    "# Select top 100 tickers by average volume\n",
    "top100_df = avg_volume_df.sort_values(\"avg_volume_million\", ascending=False).head(100)\n",
    "\n",
    "fig_volume = px.bar(\n",
    "    top100_df,\n",
    "    x=\"ticker\",\n",
    "    y=\"avg_volume_million\",\n",
    "    labels={\"avg_volume_million\": \"Avg. daily vol. (USDm)\", \"ticker\": \"Ticker\"},\n",
    "    color=\"avg_volume_million\",\n",
    "    color_continuous_scale=px.colors.sequential.Plasma\n",
    ")\n",
    "\n",
    "fig_volume.update_layout(\n",
    "    font=dict(family=\"Calibri\", size=20),\n",
    "    plot_bgcolor=\"white\",\n",
    "    paper_bgcolor=\"white\",\n",
    "    margin=dict(t=10),\n",
    "    xaxis=dict(showgrid=True, gridcolor=\"#E5E5E5\", tickangle=45),\n",
    "    yaxis=dict(showgrid=True, gridcolor=\"#E5E5E5\", zeroline=False, showline=True, linecolor=\"black\", type=\"log\")\n",
    ")\n",
    "\n",
    "fig_volume.write_html(\"avg_daily_volume_2020_log.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b6430b4-7d1e-4853-b41d-7dd6e8a21e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Sector by volume\n",
    "# -------------------------------\n",
    "\n",
    "# Merge top 100 tickers with sector info\n",
    "top100_sector_df = top100_df.merge(df[[\"ticker\", \"sector\"]], on=\"ticker\", how=\"left\")\n",
    "\n",
    "# Compute total average daily volume per sector\n",
    "sector_volume = top100_sector_df.groupby(\"sector\")[\"avg_volume_million\"].agg(['sum', 'mean', 'count']).reset_index()\n",
    "sector_volume = sector_volume.sort_values(\"sum\", ascending=False)\n",
    "\n",
    "fig_sector_volume = px.bar(\n",
    "    sector_volume,\n",
    "    x=\"sector\",\n",
    "    y=\"sum\",\n",
    "    color_discrete_sequence=[\"#4C72B0\"]\n",
    ")\n",
    "\n",
    "fig_sector_volume.update_layout(\n",
    "    showlegend=False,\n",
    "    font=dict(family=\"Calibri\", size=20),\n",
    "    plot_bgcolor=\"white\",\n",
    "    paper_bgcolor=\"white\",\n",
    "    margin=dict(t=10),\n",
    "    xaxis=dict(showgrid=False, showline=True, linecolor=\"black\", tickangle=45),\n",
    "    yaxis=dict(showgrid=True, gridcolor=\"#D3D3D3\", zeroline=False, showline=True, linecolor=\"black\"),\n",
    "    xaxis_title=\"Sector\",\n",
    "    yaxis_title=\"Total avg. daily vol. (USDm)\"\n",
    ")\n",
    "\n",
    "fig_sector_volume.update_traces(textposition=\"outside\")\n",
    "fig_sector_volume.write_html(\"top_sectors_by_avg_volume_academic.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
